
1.Data Collection
==========================================

Machine learning cycle 
-----------------------------------------
FETCH DATA -> CLEAN -> PREPARE -> Traing Model -> Evaluate Model -> Deploy Production -> Monitor & Evaluate -> FETCH DATA
GENEREATE EXAMPLE DATA -> Deploy The Model
  
*No matter how we get the data , we are building a repository. We must find a way  to congregate the data into 
single data repository.

General Data Terminology
-----------------------------------------
*Datasets - The data we use in machine learning is usually defined as dataset. dataset is collection of data.
*Structured data - which has a deined schema- includes attribute names and thier datatypes.Relational tables 
*Unstructured data - No defined scheme. Images, video audio etc
*Semi structured- To unstructured for relationsal data - CSV, JSON, XML.

Databases -  Databases are repositories for Traditional Relational DBs
Data Warehouse - some processes will takes place before storing 
Data Lake - Stores massive amount of unstructured data.No process before storing.

Machine Learning Terminologies
------------------------------------------
*Labeled Data - Data where we already know where the target data is. Supervised learning
*Unlabeled Data - Data that has been collected with no target attribute. Unsupervised learning

*Categorical features - That are associated with a group. Qualitative, Discrete
*Continuos feature - values that are expressed as measurable number. Quantitative, Infinite

*Text data - 
*Ground truth data - data that has succefully labbeled and can be trusted as truth data.
*Image data - 
*Time series data - data that changes with time.

AWS Data Stores 
----------------------------------------
*S3 -object based storage,  0-5TB single object size , unlimited storage, Go to place for ML data
*RDS - Traditional relational DBs
*DynamoDb- No relational db, key value pair. 
*Redshift- Data warehousing solution. Any kind of data. BI tools can used on warehouse.Redshift spectrume allows to query s3 data.
*Document DB- Mongodb compitible AWS db.
*TimeStream- Time series data - stock market data.



AWS Helper Tools
----------------------------------------
*EMR- Elastic Map Reduce - Its Fully managed hadoop based cluster
*Athena - Serverless paltform that allows to sql queries on s3 data.


Redshift Spectrurm vs Athena 
-----------------------
*Both query on s3
*RS needs Redshift cluster , Athena does not need
*RS is made for existing redshift customers and Athena is for new customers quickly want to query s3 data.


2. Streaming Data Collection 
=======================================

Kinesis
------------------
Kinesis data Streams
--------------------------------
*Gets data from producers, different devices and applications(social medias, video games, logs)
*Stream data stored in shards 
*Data consumers will process or analyze data(EC2, Lambda, Kinesis data analytics)
*Can be then stored in S3 or dynamodb etc
*Shards-Each shard consists of a sequence of data records. These can be ingested 1000 records per second. Defualt limit is 500 can be changed to 500.
  A data record is the unit of data captured.
  -Sequence number-Traincar
  -Partition key - train id
  -data blob - passangers
-Retention period  for data records are 24 hours after creation can be changed to 365 days.

*KPL- Kinesis Producer Library - easy to use library that allows you to write to a Kinesis Data Stream.
*KCL-K Client Library  - Integrate directly with KPL for consumer applications to consume and process data from Kinesis data stream.
*AWS SDK - Low level api similiar to KPL and KCl.

Kinesis data firehouse
---------------------------------------
-No need to worrky about shards and final goal is to store data in some data stores
-No retention period is available.


Kinesis Video Streams
Kinesis Data Analytics



3. Data Preparation
================================================
Categorical Encoding- Turning text into numbers
-------------------------
*Changing category values in our dataset to numbers
*We have some alogrithms to transform the data
*categorical value = categorical feature = discreate feature
*Nominal - order does not matter (ex- color, true/false), ordinal- order matters (ex-size)
*Encoding Ordinal attribute to numbers are easy
*Encoding Nominal attribute to numbers are not  a good idea. So One-hot encoding comes into picture.
*One hot encoding is not good when there are huge number of categories.



Feature Engineering
---------------------------------
*Text Feature Engineering
-Tranforming the text within our dataset
-Bag of words - converting it into statistical data
-N-Gram 
-Orthogonal Sparse Biagram(OSB)- 


*Numeric Feature Engineering
*Other Feature Engineering




4. Data Analysis and Visualization
========================================
- Amazon Quicksight - It is a business Intelligence tool helps to visualize data from the given dataset.
*Relationships
*Comparisions
*Distributions
*Compositions









Handling Missing Values

